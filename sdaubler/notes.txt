
SDL_Suface vs SDL_Texture
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

1 is for CPU graphics, other is for GPU graphics *[1]
	texture is hardware rendering
	surface is software

So, I presume I need to create a surface w/ the image data I have, then use SDLs apis to convert that surface into a texture?
	just like when using SDL_Image or something like that

[1]: https://stackoverflow.com/questions/21392755/difference-between-surface-and-texture-sdl-general


SDL_Surface
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

[docs][https://wiki.libsdl.org/SDL2/SDL_Surface]

format is specified in SDL_PixelFormat


SDL_PixelFormat
==================================================

[docs][https://wiki.libsdl.org/SDL2/SDL_PixelFormat]

defines how the Surface's pixel data is formatted

? why does the pixelFormat store *both* bitsPerPixel & bytesPerPixel ??

? It doesnt say that there are any restrictions for the bitdepth of the pixel.
	BUT! when using createSurface & trying combinations, some result in failure status
		
	observed: (WITH CreateRGB_Surface)
		bitmasks > bitDepth-will-fit cause error

		bitmasks wasting space will cause error
			ex: 16bit: 0x3000, 0x0300, 0x0030, 0x0003

		bitdepth == 8, presumably because this is described as automatically indexed mode?


? I dont udnerstand the "loss" values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	observed:
		when observing 32-bit (8,8,8,8) sample format, losses are all 0.

		16-bit (4,4,4,4) format, losses are all 4


SDL_CreateRGB_Surface
==================================================

[docs][https://wiki.libsdl.org/SDL2/SDL_CreateRGBSurface]
